from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from load_vectorstore_chain import rag_full_chain
import dotenv
import openai
import os

# âœ… .envì—ì„œ OpenAI API í‚¤ ë¡œë“œ
dotenv.load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

app = FastAPI()

# âœ… CORS ì„¤ì • (í”„ë¡ íŠ¸ ì—°ê²° í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],           # ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” ë„ë©”ì¸ ì§€ì • ê¶Œì¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# âœ… ìš”ì²­ ë°”ë”” ì •ì˜
class Question(BaseModel):
    question: str

# âœ… RAG ì²´ì¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„± API
@app.post("/chat/full")
async def chat_full(q: Question):
    # ğŸ”½ í˜„ì¬ëŠ” GPT-4o ê¸°ë°˜ rag_full_chainì„ ì‚¬ìš©
    answer = rag_full_chain.invoke(q.question)
    return {"answer": answer}

# âŒ ì´ì „ Ollama ê¸°ë°˜ ì§ì ‘ í˜¸ì¶œ ë°©ì‹ì€ ì•„ë˜ì™€ ê°™ì´ ì£¼ì„ ì²˜ë¦¬í•¨
# from langchain_ollama import ChatOllama
# ollama_model = ChatOllama(model="yeon")
# answer = ollama_model.invoke(q.question)
